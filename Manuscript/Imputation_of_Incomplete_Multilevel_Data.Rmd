---
documentclass: jss
author:
  - name: Hanne I. Oberman
    affiliation: |
      | Methodology and Statistics
      | Utrecht University
    address: |
      | Padualaan 14
      | 3584 CH Utrecht
    email: \email{h.i.oberman@uu.nl}
    url: https://hanneoberman.github.io/
  - name: "Johanna MuÃ±oz"
    affiliation: |
      | Julius Center for Health Sciences and Primary Care, 
      | University Medical Center Utrecht, Utrecht University, 
      | Utrecht, The Netherlands \AND
  - name: Thomas P. A. Debray
    affiliation: |
      | Julius Center for Health Sciences and Primary Care, 
      | University Medical Center Utrecht, Utrecht University, 
      | Utrecht, The Netherlands 
  - name: Gerko Vink
    affiliation: |
      | Methodology and Statistics
      | Utrecht University \AND
  - name: Valentijn M. T. de Jong
    affiliation: |
      | Julius Center for Health Sciences and Primary Care, 
      | University Medical Center Utrecht, Utrecht University, 
      | Utrecht, The Netherlands 
      | Data Analytics and Methods Task Force, 
      | European Medicines Agency, 
      | Amsterdam, The Netherlands

title:
  formatted: "Imputation of Incomplete Multilevel Data with \\proglang{R}"
  plain:     "Imputation of Incomplete Multilevel Data with R"
  short:     "Multilevel \\pkg{mice}"
abstract: >
  This tutorial illustrates the imputation of incomplete multilevel data with the \proglang{R} packackage \pkg{mice}. Our scope is only simple multilevel models, to show how imputation can yield less biased estimates from incomplete clustered data. More complex models can be accomodated, but are outside the scope of this paper. Incomplete multilevel data requires careful consideration of the missing data problem and analysis strategy. In this tutorial, we focus on a popular strategy for accommodating missingness in multilevel data: replacing the missing data with one or more plausible values, i.e., imputation.Imputation separates the missing data problem from the main analysis and the completed data can be analyzed as if it has been fully observed.This tutorial illustrates the imputation of incomplete multilevel data with the statistical programming language R. We aim to show how imputation can yield less biased estimates from incomplete clustered data. We provide practical guidelines and code snippets for different missing data situations, including non-ignorable missingness mechanisms. For brevity, we focus on multilevel imputation using chained equations with the R mice package and its adjacent packages.
keywords:
  # at least one keyword must be supplied
  formatted: [missing data, multilevel, clustering, "\\pkg{mice}", "\\proglang{R}"]
  plain:     [missing data, multilevel, clustering, mice, R]
preamble: >
  \usepackage{amsmath}
header-includes: 
 - \usepackage{graphicx} 
 - \usepackage{mathtools}
 - \usepackage{ulem}
 - \usepackage{xcolor}
output: 
    rticles::jss_article:
      keep_tex: yes
      number_sections: yes
    # word_document: default
bibliography: ../References/multilevelmice.bib
editor_options: 
  chunk_output_type: inline
---

```{r, setup, include=FALSE, eval=TRUE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
options(prompt = 'R> ', continue = '+ ')

# environment
set.seed(123)
library(dplyr)
library(ggplot2)
```


# Introduction

## Multilevel data

Many datasets include individuals that are clustered together, for example in geographic regions, or even different studies. In the simplest case, individuals (e.g., students) are nested within a single cluster (e.g., school classes). More complex clustered structures may occur when there are multiple hierarchical levels (e.g., students in different schools or patients within hospitals within regions across countries), or when the clustering is non-nested (e.g., electronic health record data from diverse settings and populations within large databases). With clustered data we generally assume that individuals from the same cluster tend to be more similar than individuals from other clusters. In statistical terms, this implies that observations from the same cluster are not independent and may in fact be correlated. If this correlation is left unaddressed, estimates of *p* values, confidence intervals even model parameters are prone to bias  [@loca01]. Statistical methods for clustered data typically adopt hierarchical models that explicitly describe the grouping of observations. These models are also known as 'multilevel models', 'hierarchical models', 'mixed effect models', 'random effect models', and in the context of time-to-event data as 'frailty models'. Table \ref{tab:clus} provides an overview of some key concepts in multilevel modeling.


Table 1: Concepts in multilevel methods

```{r clus0,  echo = FALSE, warning = FALSE,cache = TRUE}
concepts <- data.frame(Concept=c("Sample units","Cluster","Hierarchical data","Level-1","Level-2","Hierarchical model","Fixed effect","Random effect","Mixed effect","ICC","Stratified intercept"),
                       Details=c("Units of the population from which measurements are taken in a sample, e.g., students.", "Variable that specify the cluster or agruppation, e.g., Classroom","Data are grouped into clusters at different levels, observations belonging to the same cluster are expected to share certain characteristics.","Variable that varies within a cluster, eg. Test score","Variable that does not vary within a cluster but between, e.g. teacher experience.","Model accounting for dependant observations relying on certain parameters ( within cluster) which in turn depend on other parameters (between cluster)" ,"Effects that are constant across all sample units, e.g. something that researchers control for and can repeat, such as  a teaching strategy (tutoring after class)","Effects that are a source of random variation in the data, and whose levels are not fully sampled. e.g. test score tendency during academic year between students due to no controlled factors such as  genetic,family history","Includes fixed and random effects, e.g. the fixed effect would be the treatment effect of a drug and the random effect would be the ID of the hospital where the patient is treated. Multilevel models typically accommodate for variability by including a separate group mean for each cluster e.g random intercept on hospitals. In addition to random intercepts, multilevel models can also include random coefficients and heterogeneous residual error variances across clusters (see e.g. @gelm06, @hox17 and @jong21).","The variability due to clustering is often measured by means of the intraclass coefficient (ICC). The ICC can be seen as the percentage
of variance that can be attributed to the cluster-level, where a high ICC would indicate that a lot of variability is due to the cluster structure.",
                                 ""))
library(kableExtra)
concepts%>%
  kable( booktabs = T, escape = F,
         caption = "Concepts in multilevel methods",
         col.names = c("Concept","Details"), align = "l")%>%
  column_spec(c(1), width = "3cm")%>%
  column_spec(c(2), width = "12cm")
```

## Missingness in multilevel data

As with any other dataset, clustered datasets may be impacted by missingness in much the same way. Several strategies can be used to handle missing data, including complete case analysis and imputation. We focus on the latter approach and discuss statistical methods for replacing the missing data with one or more plausible values. Imputation separates the missing data problem from the analysis and the completed data can be analyzed as if it were completely observed. It is generally recommended to impute the missing values more than once to preserve uncertainty due to missingness and to allow for valid inferences (c.f. Rubin 1976).

With incomplete clustered datasets we can distinguish between two types of missing data: sporadic missingness and systematic missingness [@resc13]. Sporadic missingness arises when variables are missing for some but not all of the units in a cluster [@buur18; @jola18]. For example, it is possible that test results are missing for several students in one or more classes. When all observations are missing within one or more clusters, data are said to be systematically missing. Sporadic missingness is visualized in Figure XYZ. 
<!-- [TODO: Refer to Figure 1 and put interpretation in the figure caption. VMTdJ: "Mention in text that this is sporadic missingness?"] -->
<!-- TODO: Provide an example for one of the case studies below.] -->
<!-- TODO: add that indicator method doesn't work with syst missing (only sporadically). There's some pro's and con's. May not differ much if the number of clusters is low. -->

<!-- Systematic missingness implies that one or more variables are never observed in a certain cluster. With sporadic missingness there may be observed data for some but not all units in a cluster [@buur18; @jola18]. We have visualized this difference in Figure 1, which shows an $n \times p$ set $\mathbf{X} = X_1, \dots, X_p$, with $n$ units distributed over $N$ clusters and $p$ variables.  -->

```{r patterns, fig.height=1.5, fig.width=4.1, fig.cap = "Sporadic missingness in multilevel data", echo=FALSE, fig.margin=TRUE}
dat <- expand.grid(rows = 1:7, cols = 1:6) %>% 
  cbind(text = c("1", "1", "2", "2", "3", "", "N", rep("", 35)),
        miss = c(rep("", 16), "NA", "NA", "", "", "", "NA", "", "", "NA", rep("", 17)))
ggplot(dat, aes(x = cols, y = rows)) +
  geom_tile(fill = "white", color = "black", size = 0.5) +
  geom_text(aes(label = text), color = "black", size = 3) +
  geom_text(aes(label = miss), color = mice:::mdc(2), family = "mono", fontface = "bold") + 
  scale_x_continuous(
    breaks = 1:6,
    labels = c("cluster", expression(X[1]), expression(X[2]), expression(X[3]), "...", expression(X[p])),
    name = NULL,
    position = "top"
  ) +
  scale_y_continuous(breaks = 1:7, labels = c(1:5, "...", "n"), name = NULL, trans = "reverse") +
  # coord_cartesian(expand = c(0,0)) +
  theme_minimal() +
  theme(panel.grid = element_blank(),
        plot.margin = unit(c(0,0,0,0), "pt"))
# TODO: use math font for n and N
```
<!-- Column $X_1$ in Figure 1 is completely observed, column $X_2$ is systematically missing in cluster 2, and column $X_3$ is sporadically missing. To analyze these incomplete data, we have to take the nature of the missingness and the cluster structure into account. For example, the sporadic missingness in $X_3$ could be easily amended if this would be a cluster-level variable (and thus constant within clusters). We could then just extrapolate the true (but missing) value of $X_3$ for unit 1 from unit 2, and the value for unit 4 from unit 3. If $X_3$ would instead be a unit-level variable (which may vary within clusters), we could not just recover the unobserved 'truth', but would need to use some kind of missing data method, or discard the incomplete units altogether (i.e., complete case analysis). Complete case analysis can however introduce bias in statistical inferences and lowers statistical power. Further, with the systematic missingness in $X_2$, it would be impossible to fit a multilevel model without accommodating the missingness in some way. Complete case analysis in that case would mean excluding the entire cluster from the analyses. The wrong choice of missing data handling method can thus be extremely harmful to the inferences.  -->

Imputation of missing data requires consideration of the mechanism behind the missingness. Rubin proposed to distinguish between data that are missing completely at random (MCAR), data that are missing at random (MAR) and data that are missing not at random (MNAR; see Table \ref{tab:miss}). For each of these three missingness generating mechanisms, different imputation strategies are warranted (@yuce08 and @hox15). We here consider the general case that data are MAR, and expand on certain MNAR situations.

Table 2: Concepts in missing data methods

| **Concept**    | **Details**                                                                |
|----------------|----------------------------------------------------------------------------|
| MCAR           | Missing Completely At Random, where the probability to be missing is equal |
|                | across all data entries                                                    |
| MAR            | Missing At Random, where the probability to be missing depends on observed |
|                | information                                                                |
| MNAR           | Missing Not At Random (MNAR), where the probability to be missing          |
|                | depends on unrecorded information, making the missingness non-ignorable    |
|                | [@rubi76; @meng1994].                                                        |
|                |                              |

<!-- [TODO: add paragraph about congeniality and smc-fcs!]  -->

<!-- Since excluding observations is not a desirable workflow, the missingness in multilevel data should be accommodated \emph{before} or _within_ the analysis of scientific interest. In this paper, we focus on the former approach: imputing (i.e., filling in) the missing data with plausible values, whereafter the completed data may be analyzed as if it were completely observed. Imputation separates the missing data problem from the scientific problem, which makes the missing data strategy very generic and popular. If each missing value is replaced multiple times, the resulting inferences may validly convey the uncertainty due to missingness [c.f. @rubi76].  -->

<!-- [TODO: clarify why clustering is relevant during imputation, and why this exposes the need for specialized imputation methods and more attention during their implementation ("thou shall not simply run `mice()` on any incomplete dataset").] -->
<!-- [TODO: Add that the more the random effects are of interest, the more you need multilevel imputation models.] [TODO: Add an overview of all possible predictor matrix values in manuscript or `ggmice` legend.] --> 


## Aim of this paper

This papers serves as a tutorial for imputing incomplete multilevel data with \pkg{mice} in \proglang{R}. \pkg{mice} has become the de-facto standard for imputation by chained equations, which iteratively solves the missingness on a variable-by-variable basis. \pkg{mice} is known to yield valid inferences under many different missing data circumstances [@buur18].

We provide practical guidelines and code snippets for different missing data situations, including non-ignorable mechanisms. For reasons of brevity, we focus on multilevel imputation by chained equations with \pkg{mice} exclusively; other imputation methods and packages [see e.g. @audi18 and @grun18] are outside the scope of this tutorial. Assumed knowledge includes basic familiarity with the \pkg{lme4} notation for multilevel models (see Table \ref{tab:mod}).

<!-- TODO: mention other packages? \pkg{jomo} and \pkg{mdmb} -->



We illustrate imputation of incomplete multilevel data using three case studies:

- `popmis` from the \pkg{mice} package [simulated data on perceived popularity, $n = 2,000$ pupils across $N = 100$ schools with data that are MAR, @mice];
- `impact` from the \pkg{metamisc} package [empirical data on traumatic brain injuries, $n = 11,022$ patients across $N = 15$ studies with data that are MAR, @metamisc];
- `obesity` from the \pkg{micemd} package [simulated data on obesity, $n = 2,111$ patients across $N = 5$ regions with data that are MNAR].

For each of these datasets, we discuss the nature of the missingness, choose one or more imputation models and evaluate the imputed data, but we will also highlight one specific aspect of the imputation workflow. 

This tutorial is dedicated to readers who are unfamiliar with multiple imputation. More experienced readers can skip the introduction (case study 1) and directly head to practical applications of multilevel imputation under MAR conditions (case study 2) or under MNAR conditions (case study 3). 

<!-- With the `popmis` data, we show how (and how not) to develop an imputation model. With the `hiv` data we focus on extending the imputation model to include Heckman-type selection-inclusion methods. With the `impact` data we provide an example of multivariate missingness in real-world data. Together, this should give enough scaffolding for applied researchers who are faced with incomplete multilevel data. -->

<!-- TODO: explicit statement about not going into workings of the methods. Galimer 2l methods. -->

## Imputation workflow
Below we provide a imputation workflow that can be used in general to impute cluster data.

### Focus on the Main Analysis

When dealing with incomplete clustered data, start by looking at your research questions and planned analysis. Pretend there are no missing data at first. This will help you understand your research hypotheses, the main statistical model, and give you insights into your data's structure (refer to the level table), variable types (e.g., confounders, auxiliary variables), and other considerations such as variable relationships, interactions or polynomial terms.

### Exploration of Available Data

Now, explore what's in your data. Use simple tools like histograms and QQ plots to understand variable distributions, plausible range of values and identify errors or outliers. Check interactions between predictors with scatter plots, look for collinearity with VIF or correlations plots, this may also help you to identify non-considered relationships that may affect the main model.  You might also consider to test assumptions related to your response variable such as variance homogeneity, independence between observations (eg. ACF, variagrams).  This will help you choose an imputation model that suits your data. In addition, the intraclass correlation coefficient (ICC) can be examined to assess cluster differences, aiding in the choice between the 2l and 1l methods for imputation.

Next, explore the missingness. Look at the \textbf{proportion of missing values} in the dataset variables. This helps find potential predictors and cut down on unnecessary variables in the imputation model. Doing this can lower the risk of multicollinearity or computational issues, especially with certain parametric imputation methods. You can also identify predictors for the imputation model by using inflow criteria to see connections between missing data in one variable and observed variables, and outflow criteria to identify connections between observed values in one variable and missing data in others.

Check the missing patterns at cluster level, this can help you to select the most appropiated imputation approach in terms of computational efficiency (e.g., simpler regression imputation versus FCS in univariate patterns). 


### Assess Estimation Procedure Robustness to Missing Data
Before diving into imputation, make sure your estimation model can handle missing data. Sometimes, simpler methods like complete case analysis might be suffice, especially if your missingness is low (usually <5%). There are scenarios in which specific Maximum Likelihood (ML) estimation methods outperform Multiple Imputation (MI) methods, for instance when the response variable is the sole incomplete variable, mixed models demonstrate robustness to missing data under the Missing at Random (MAR) assumption and with a correct variance-covariance specification.\cite{Molenberghs_2007}.


### Pre-imputation
Clean up your dataset before the imputation process. Keep initially the essential variables for your main model, but include extra variables if needed for specific procedures during analysis (e.g. confounders on balancing procedures). Think about adding other useful variables, even if they're not in the main model, such as instrumental or auxiliary variables which might boost your parameter estimates, especially if they're associated to the probability of missingness for some incomplete variables.

Figure out if you can directly impute incomplete variables by just use deductive imputation. This involves inferring missing values based on logicallogical connections between variables. It's especially handy for variables that depend on each other, like calculating BMI from weight and height.

Deductive imputation is also useful for getting values for level-1 variables from level-2 ones, like in Individual Participant Data (IPD). In these situations, you can guess missing information from metadata or by using or through deduction based on time or protocols. For example, you could deduce missing test values for patients who have passed away.

### Setting Imputation Model

##### Clustering Inclusion

When it comes to handling clusters during the imputation process, you've got a few options. You can use a cluster indicator variable, run separate imputation for each cluster, or go for a simultaneous imputation method that takes clustering into account \cite{eddings}.

Which strategy you choose depends on the assumptions in your main analysis and the limitations of your data. If your analysis do not use a hierarchical model (like a descriptive approach) and you have a small number of clusters with lots of observations in each, using a cluster indicator or separate imputation might be the way to go \cite{graham2009}. Conversely, if you have more clusters or fewer observations per cluster, you might want to try a simultaneous hierarchical imputation model \cite{Allison_2002}.

In a hierarchical imputation model, random effects model the correlations between observations within clusters, making it possible to estimate even with a small number of observations per cluster. There are various proposed multiple imputation models based on hierarchical models, each with its own set of assumptions \cite{audigier}.

If your analysis uses a hierarchical model, make sure the assumptions of your imputation model match up. For example, using a cluster indicator approach may lead to bias estimates if your model is based on a hierarchical structure \cite{taljaard2008,speidel2018}. Even if you prefer an imputation strategy that aligns with your main model, check if it suits your data; sometimes simpler strategies can give unbiased estimates in certain scenarios \cite{bailey2020}.


#### Choice of Individual Imputation Methods
Start by choosing the imputation model for each incomplete variable in your dataset. The mice package suggests methods based on variable types for non-clustered variables, and for the cluster ones, you can use the micemd package's find.defaultMethod() function. This function selects from different 2l imputation methods based on cluster size and the proportion of missing data in each cluster.

Besides the package-defined imputation methods, you can specify custom methods using the "I formula". This lets you calculate deterministic variables during the imputation or tweak imputation methods based on specific conditions, like conditioning the imputation model to the level of an incomplete covariate (e.g., a pregnancy test for females).

#### Model Specification

The imputation model must be congenial with the main model \cite{meng1994}. Congeniality issues arise when the imputation model and the main model make different assumptions, often due to the omission of a polynomial or interaction term or the use of transformed variables.

The imputation model can incorporate additional terms compared to the main model without causing compatibility issues. For instance, it's recommended to include the outcome variable in the imputation model for prediction variables \cite{moons2006a}. In cases where the outcome is time-to-event, the Nelson-Aalen estimate of the time to the event should be added as a covariate in the imputation model [REF]. Additionally, including auxiliary variables, even if not part of the main model, can be linked to the probability of missingness, improving the likelihood of meeting the Missing at Random (MAR) assumption and enhancing estimation efficiency \cite{hardt2012a}.

Imputation models are specified on a variable basis, either using a prediction matrix in the pred parameter or through a list of formulas in the formula parameter. In the prediction matrix option, the type of each predictor variable is specified for each incomplete variable (see table). 

\begin{align}
y_{ij} =& (\beta_0 + b_{0_i})+ \beta_1x_{1_{ij}}+\beta_2x_{2_{i.}} \nonumber \\
+& (\beta_3 +  b_{3_i})x_{3_{ij}} \nonumber \\
+& \beta_4x_{4_{ij}} + \beta_{m4}\overline{x_{4_{i.}}}\nonumber \\
+& (\beta_5 +  b_{5_i})x_{5_{ij}} +\beta_{m5}\overline{x_{5_{i.}}}\nonumber \\
\end{align}


\begin{tabular}{ll}
     \toprule
Type & Definition \\
  \midrule
-2  & Cluster variable, in this case the one defined by j index  \\
 1  & Fixed variable, e.g., level-1  $x_{1_{ij}}$ or  level-2 $x_{2_{i.}}$ \\
 2  & Random variable, e.g., $x_{3_{ij}}$\\
 3  & *Fixed variable with cluster mean $x_{4_{ij}}$\\
 4  & *Random variable with cluster mean $x_{5_{ij}}$\\
-3. & Random variable only included on selection model (Heckman model)\\
-4. & Random variable only included on main model (Heckman model)\\
      \bottomrule
\end{tabular}

* 3 and 4 type have been advised to used as the inclusion of the means of the cluster is beneficial on FCS \cite{mistler2017}


Recipes have been proposed for imputing incomplete level-1 and level-2 variables for hierarchical models \cite{buuren2018a} ($\S$ 7.10), which can be convenient to follow when dealing with numerous variables (interaction terms at different levels) and models with many random effects that are prone to convergency problems due to overspecification in the imputation model. These rules were designed to ensure compatibility among the conditionally specified imputation models and congeniality between the imputation and the main model. However, they may not be applicable to all 2l imputation methods; for instance, the 2l.2stage imputation methods of the micemd package only allow the inclusion of random predictor variables (2).

On the other hand, the formula option is useful in specifying complex imputation models with polynomial terms or interactions and compared with the prediction matrix method do not requires the inclusion of additional terms as Just Another Variable (JAV) \cite{buuren2018a}($\S$ 6.4).

For some interaction terms, it has been suggested, for instance, for treatment interaction effects, to conduct separate imputation by treatment group \cite{zhang2023}. Additionally, it can be used imputation models based on random forest or deep learning that can handle interaction and non-linear terms without requiring the explicit specification of an imputation model. 


### Post-Imputation 

During the imputation process, certain issues may arise that halt the process. In hierarchical model imputations, many issues are related to over fitting the imputation model. To troubleshoot, it is recommended to inspect the imputation log file for variables causing problems.  One approach to address this is to reduce the number of predictors, by using step by step the previous referred recepies or by using functions like quickpred. Another option is to consider variable transformations, such as scaling when model is invariant to linear transformations e.g., random intercept models.  Also adjusting the level of the hierarchical model (e.g., using a homogeneous variance imputation method or a 1l model) can also be beneficial. 

It is crucial to check the range of imputed variables, as excessively large imputed values for one predictor may trigger convergence issues in other variables. To tackle this, including post-processing specifications on problematic variables or utilizing imputation models like Predicted Mean Matching (PMM) can ensure that imputed values align with observable values.

In some situations, adopting a separate imputation strategy might be worth considering. For example, in analyses involving multiple endpoints, conducting distinct imputation processes for each endpoint could be more effective than a unified imputation approach.

### Convergence and Sensitivity Analysis

Before starting into the analysis of each imputed dataset, it is crucial to validate the convergence of the imputation process. This is commonly accomplished through trace plots that depict the mean and variance of the incomplete variables across iterations. These plots serve to uncover potential circular issues or the need for additional iterations. Additionally,  it is also important to verify that imputed values fall within a plausible range and also to check the distribution of imputed variables, ensuring that the imputed variable distribution aligns with the distribution of observed values (under the MAR assumption). An alternative approach involves assessing the prediction accuracy of the imputation method \cite{cai2023}.

While the majority of Multiple Imputation by Chained Equations (MICE) methods are based on Missing at Random (MAR) assumptions, field expert input may suggest that the Missing Not at Random (MNAR) mechanism could be plausible for certain variables. An MNAR variable is one in which the probability of missingness depends on an unobservable variable. This can occur when missingness is associated with the incomplete value itself (self-marking) or when there is an unobserved variable linked to both the value and the probability of missingness of the incomplete value (indirectly non-informative). Specifically, for the indirectly non-informative case in hierarchical datasets, imputation methods based on the Heckman method can be considered.\cite{hammon2020,hammon2022,munoz2023}


<!-- With the `popmis` data, we show how (and how not) to develop an imputation model. With the `hiv` data we focus on extending the imputation model to include Heckman-type selection-inclusion methods. With the `impact` data we provide an example of multivariate missingness in real-world data. Together, this should give enough scaffolding for applied researchers who are faced with incomplete multilevel data. -->

<!-- TODO: explicit statement about not going into workings of the methods. Galimer 2l methods. -->

## Setup
<!-- * TODO: CREATE A SINGLE ONLINE DEPARTURE POINT ON GITHUB? -->

<!-- [TODO: Add environment info, seed and version number(s) somewhere.]  -->

Set up the R environment and load the necessary packages:
```{r env, message=FALSE, warning=FALSE}
set.seed(123)         # for reproducibility
library(mice)         # for imputation
library(miceadds)     # for additional imputation routines
library(ggmice)       # for incomplete/imputed data visualization
library(ggplot2)      # for visualization
library(dplyr)        # for data wrangling
library(lme4)         # for multilevel modeling
library(mitml)        # for multilevel parameter pooling
library(micemd)       # for case study data and imputation cf. heckman models
library(metamisc)     # for case study data
library(broom.mixed)  # for multilevel estimates
```


TODO: add table with predictor matrix values

- -2 = cluster variable 
- 1 = overall effect
- 3 = overall + group-level effect 
- 4 = individual-level (random) and group-level (fixed) effect 
 
# Case study I: popularity data

<!-- [TODO: explain case study] -->

In this section we will go over the different steps involved with imputing incomplete multilevel data with the R package mice. We consider the simulated `popmis` dataset, which included pupils ($n = 2000$) clustered within schools ($N = 100$). The following variables are of primary interest: 

  - `school`,     school identification number (clustering variable);
  - `popular`,    pupil popularity (self-rating between 0 and 10; unit-level);
  - `sex`,	      pupil sex (0=boy, 1=girl; unit-level);
  - `texp`,	      teacher experience (in years; cluster-level).

The research objective of the `popmis` dataset is to predict the pupils' popularity based on their gender and the experience of the teacher. The analysis model corresponding to this dataset is multilevel regression with only random intercepts. The outcome variable is `popular`, which is predicted from the level-1 (student) variable `sex` and the level-2 (school) variable `texp`. Given the $j$-th student belonging to the $i$-th school, the main model can be formulated as:

$$ popular_{ij} = (\beta_0 + b_i) + \beta_1sex_{ij}+ \beta_2texp_{ij}+\epsilon_{ij}$$
where $\epsilon_{ij}$ corresponds to the error term.

<!-- TODO: choose sex or gender -->

<!-- $$ -->
<!-- \texttt{popular} \sim  \texttt{1 + sex + texp + sex:texp + (1 + sex | school)} -->
<!-- $$ -->
```{r}
mod <- popular ~ 1 + sex + (1 | school)
```

The estimated effects in the complete data are presented in Table XYZ. We consider the associations in the full data set to be the true associations.  

```{r include=FALSE}
# data
popcomp <- foreign::read.spss("../Data/popular.sav", to.data.frame = TRUE, use.value.labels = FALSE) %>% 
  mutate(school = SCHOOL,
         popular = POPULAR,
         sex = SEX,
         .keep = "none")

est_true <- lme4::lmer(mod, popcomp) %>% broom.mixed::tidy()
# testEstimates(as.mitml.result(fit_ignored), var.comp = TRUE)
```


<!-- TODO: True effect? We have not discussed any effects yet. Maybe say that "we consider the associations in the full data set to be the true associations''. And mention association instead of effect (effect has a causal interpretation, association can be either causal or predictive) -->

Load the data into the environment and select the relevant variables:

```{r}
popmis <- popmis[, c("school", "popular", "sex")] 
```

First we plot the pattern of missing data within categories of the relevant variables. Plot the missing data pattern:

```{r pop_pat, fig.cap = "Missing data pattern in the popularity data", fig.margin=TRUE}
plot_pattern(popmis)
```

The missingness is univariate and sporadic, which is illustrated in the missing data pattern in Figure \ref{fig:pop_pat}. 

The ICC in the incomplete data is `round(icc(popular ~ as.factor(school), data = na.omit(popmis)), 2)`. This tells us that the multilevel structure of the data should probably be taken into account. If we don't, we'll may end up with incorrect imputations, biasing the effect of the clusters towards zero.

To develop the best imputation model for the incomplete variable `popular`, we need to know whether the observed values of `popular` are related to observed values of other variables. Plot the pair-wise complete correlations in the incomplete data:
<!-- Excelent, but i dont know if you can refer to the outflux criteria here or elsewhere, cuz in practice it is used to select the predictors in the imputation model from dataset with a lot of features-->

```{r pop-corr}
plot_corr(popmis)
```

This shows us that  `sex`  may be a useful imputation model predictor. Moreover, the missingness in `popular` may depend on the observed values of other variables. 

<!-- We'll highlight one other variable to illustrate, but ideally one would inspect all relations. The questions we'll ask are: 'Does the missing data of pupil popularity (`popular`) depend on observed teacher popularity (`texp`)?'. This can be evaluated statistically, but visual inspection usually suffices. We'll make a histogram of `texp` separately for the pupils with known popularity and missing popularity. -->

<!-- Plot the histogram for teacher experience conditional on the missingness indicator of `popular`: -->

```{r pop-hist}
# ggmice(popmis, aes(sex)) +
#   geom_histogram(fill = "white") +
#   facet_grid(. ~ is.na(popular), scales = "free", labeller = label_both)

ggplot(popmis, aes(y = popular, group = sex)) +
  geom_boxplot() + 
  theme_classic()
```

<!-- This shows us that there are no apparent differences in the distribution of `texp` depending on the missingness indicator of `popular` (t = r t.test(popmis$texp ~ is.na(popmis$popular)) %>% broom::tidy(.) %>% .[, c("statistic", "p.value")] %>% round(., 3) %>% unlist() %>% paste(collapse = ",  p = ")). -->
<!--  [TODO: think about what is a meaningful rule of thumb to signal that the user should be worried?] -->
 
 
<!-- ### Complete case analysis (not recommended) -->

<!-- Complete case analysis ignores the observations with missingness altogether, which lowers statistical power and may even introduce bias in MCAR situations.  -->

<!-- ```{r pop-cca, echo=FALSE} -->
<!-- est_cca <- lme4::lmer(mod, popmis) %>% broom.mixed::tidy() -->
<!-- # as.matrix(est_cca$estimate) - as.matrix(est_true) -->
<!-- est_cca$estimate - est_true$estimate -->
<!-- ``` -->


### Imputation ignoring the cluster variable (not recommended)

The first imputation model that we'll use is likely to be invalid. We do *not* use the cluster identifier `school` as imputation model predictor. With this model, we ignore the multilevel structure of the data, despite the high ICC. This assumes exchangeability between units. We include it purely to illustrate the effects of ignoring the clustering in our imputation effort. 
<!-- We'll use the default imputation methods in `mice()` (predictive mean matching to impute the continuous variables and logistic regression to impute binary variables).  -->

Create a methods vector and predictor matrix for `popular`, and make sure `school` is not included as predictor:

```{r pop-ignored-pred, echo=TRUE, message=FALSE, warning=FALSE}
meth <- make.method(popmis) # methods vector
pred <- quickpred(popmis)   # predictor matrix
plot_pred(pred)
```

Impute the data, ignoring the cluster structure:
```{r pop-ignored-imp, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
imp <- mice(popmis, pred = pred, print = FALSE)
```

Evaluate the convergence of the algorithm:
```{r pop-ignored-conv, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
plot_trace(imp)
```
<!-- [TODO: remove the broom.mixed output, use mitml only] -->

Analyze the imputations:
```{r pop-ignored-fit, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
fit <- with(imp, 
            lmer(popular ~ 1 + sex  + (1 | school))) 
```

Print the estimates:

<!-- TODO: explain what this function does -->

```{r pop-print, eval=FALSE}
testEstimates(as.mitml.result(fit), extra.pars = TRUE)
```


### Imputation with the cluster variable as predictor (not recommended) 

We will now use `school` as a predictor to impute all other variables. This is still not recommended practice, since it only works under certain circumstances and results may be biased [@drec15; @ende16]. But at least, it includes some multilevel aspect. This method is also called 'fixed cluster imputation', and uses N-1 indicator variables representing allocation of N clusters as a fixed factor in the model [@reit06; @ende16]. Colloquially, this is 'multilevel imputation for dummies'. 

<!-- [TODO: Add that it doesn't work with systematic missingness (only with sporadic). There's some pros and cons, and it may not even differ much if the number of clusters is low.] -->

<!-- A fixed cluster model is better if we have data with all the clusters in the population and we are interested in knowing the effect of a treatment on the specific cluster eg. Cluster No.123. If the population is too large (10000 schools) and we have a sample (200 schools) then an random cluster is better and saves us degrees of freedom because some of the parameters are random variables. Here we have to say that the imputation model should be more in line with the analysis model (that defines in part how the cluster is included) and consider how the estimation of the imputation model could be affected when N is too large (as we add N dummies in the fixed cluster model) or if ni is too small in some clusters, so the model can not be estimated in them.. -->


```{r pop_predictor, message=FALSE, warning=FALSE, cache=TRUE}
# adjust the predictor matrix
pred["popular", "school"] <- 1 
plot_pred(pred)

# impute the data, cluster as predictor
imp <- mice(popmis, pred = pred, print = FALSE)
```

Evaluate the convergence of the algorithm:
```{r pop-predictor-conv, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
plot_trace(imp)
```

Analyze the imputations:
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
fit <- with(imp, 
            lmer(popular ~ 1 + sex + (1 | school))) 
```

Print the estimates:
```{r, eval=FALSE}
testEstimates(as.mitml.result(fit), extra.pars = TRUE)
```

### Imputation with multilevel model

```{r pop_multilevel, message=FALSE, warning=FALSE, cache=TRUE}
# adjust the predictor matrix
pred["popular", "school"] <- -2 
plot_pred(pred)

# impute the data, cluster as predictor
imp <- mice(popmis, pred = pred, print = FALSE)
```

Evaluate the convergence of the algorithm:
```{r pop-multilevel-conv, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
plot_trace(imp)
```

Analyze the imputations:
```{r, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
fit <- with(imp, 
            lmer(popular ~ 1 + sex + (1 | school))) 
```

Print the estimates:
```{r, eval=FALSE}
testEstimates(as.mitml.result(fit), extra.pars = TRUE)
```



# Case study II: IMPACT data (syst missingness, pred matrix)

<!-- [TODO: check if there is systematic missingness in this dataset, if not make Marshall Computerized Tomography classification (ct) systematically missing.] -->

We illustrate how to impute incomplete multilevel data by means of a case study: `impact` from the \pkg{metamisc} package [empirical data on traumatic brain injuries, $n = 11,022$ units across $N = 15$ clusters, @metamisc]. 
<!-- [TODO: add more info about the complete data.]  -->
The `impact` data set contains traumatic brain injury data on $n = 11022$ patients clustered in $N = 15$ studies with the following 11 variables:

  - `name` Name of the study,
  - `type` Type of study (RCT: randomized controlled trial, OBS: observational cohort),
  - `age` Age of the patient,
  - `motor_score` Glasgow Coma Scale motor score,
  - `pupil` Pupillary reactivity,
  - `ct` Marshall Computerized Tomography classification, 
  - `hypox` Hypoxia (0=no, 1=yes),
  - `hypots` Hypotension (0=no, 1=yes),
  - `tsah` Traumatic subarachnoid hemorrhage (0=no, 1=yes),
  - `edh` Epidural hematoma (0=no, 1=yes),
  - `mort` 6-month mortality (0=alive, 1=dead).

<!-- [TODO: make ct levels one var? also shows that you don't always need random effects everywhere?] -->
In this dataset all the variables are level-1 (patient), except by the level-2 (study) type variable.  The analysis model for this dataset is a prediction model with `mort` as the outcome. In this tutorial we'll estimate the adjusted prognostic effect of `ct` on mortality outcomes. The estimand is the adjusted odds ratio for `ct`, after including `type`, `age` `motor_score` and `pupil`. Therefore the main model for the $i$-th patient from the $j$-th study can be described as:

$$ mort_{ij} = (\beta_0 + b_i) + \beta_1type_{.j}+ \beta_2age_{ij}+ \beta_3motorscore_{ij} + \beta_4pupil_{ij} + \beta_5ct_{ij}+\epsilon_{ij}$$
where $\epsilon_{ij}$ corresponds to the error term.

```{r mod}
mod <- mort ~ type + age + motor_score + pupil + ct + (1 | name) 
```

Note that variables `hypots`, `hypox`, `tsah` and `edh` are not part of the analysis model, and may thus serve as auxiliary variables for imputation. 

The `impact` data included in the \pkg{metamisc} package is a complete data set. The original data has already been imputed once (Steyerberg et al, 2008). For the purpose of this tutorial we have induced missingness (mimicking the missing data in the original data set before imputation). The resulting incomplete data can be accessed from [zenodo link to be created](https://zenodo.com). 

Load the complete and incomplete data into the R workspace:
```{r data, eval = FALSE}
data("impact", package = "metamisc")      # complete data
dat <- read.table("link/to/the/data.txt") # incomplete data
```

```{r data-actually, include=FALSE}
data("impact", package = "metamisc")
dat <- readRDS("../Data/impact_incomplete.RDS")
```

<!-- visualized in Figure \ref{}. -->

```{r forest, echo=FALSE}
# fits <-
#   purrr::map_dfr(
#     unique(impact$name),
#     ~ {
#       glm(
#         mort ~ 1 + age + motor_score + pupil + ct,
#         family = "binomial",
#         data = filter(impact, name == .x)
#       )
#     } %>%
#       broom::tidy(conf.int = TRUE, exponentiate = TRUE) %>%
#       .[, c("term", "estimate", "conf.low", "conf.high")] %>%
#       cbind(name = as.character(.x), .)
#   ) %>% filter(term %in% c("ctIII", "ctIV/V"))
# 
# n <- impact %>% group_by(name) %>% 
#   tally()
# 
# fits %>% left_join(n, by = "name") %>% 
#   ggplot(aes(y = name, x = estimate, color = term, linewidth = n)) +
#   geom_vline(xintercept = 1, linewidth = 1.5, color = "grey95") +
#   geom_linerange(aes(y = name, xmin = conf.low, xmax = conf.high, linewidth = 0.5), position = position_dodge(width = 0.5)) +
#   geom_point(shape = 20, position = position_dodge(width = 0.5)) +
#   labs(y = "Study", x = "Adjusted odds ratio for ct", color = NULL) +
#   theme_classic() +
#   scale_size(guide = 'none')
# # TODO: add heterogeneity in distribution of ct -> prevalence in ct across clusters with CI
# # TODO: missingness prop inside clusters
# # TODO: make row height proportional to pattern frequency
```

We will use the complete data estimates as comparative truth in this tutorial. The estimated effects in the complete data are presented in Table XYZ. 
```{r impact_truth, include=FALSE, echo=TRUE}
fit <- glmer(mod, family = "binomial", data = impact) # fit the model
tidy(fit, conf.int = TRUE, exponentiate = TRUE)       # print estimates
```

<!-- [TODO: show how much variance there is after different methods] -->

<!-- [TODO: add ICC before/after imputation and interpret: This tells us that the multilevel structure of the data should probably be taken into account. If we don't, we'll may end up with incorrect imputations, biasing the effect of the clusters towards zero.] -->

<!-- [TODO: add descriptive statistics of the complete and incomplete data.] -->

## Missingness 

To explore the missingness, it is wise to look at the missing data pattern. The ten most frequent missingness patterns are shown:
```{r pattern, fig.height=7.1}
plot_pattern(dat, rotate = TRUE, npat = 10L)  # plot missingness pattern
```

This shows that we need to impute `ct` and `pupil`.

To develop the best imputation model, we need to investigate the relations between the observed values of the incomplete variables and the observed values of other variables, and the relation between the missingness indicators of the incomplete variables and the observed values of the other variables. To see whether the missingness depends on the observed values of other variables, we can test this statistically or use visual inspection (e.g. a histogram faceted by the missingness indicator).

We should impute the variables `ct` and `pupil` and any auxiliary variables we might want to use to impute these incomplete analysis model variables. We can evaluate which variables may be useful auxiliaries by plotting the pairwise complete correlations:
```{r impact_corr}
plot_corr(dat, rotate = TRUE) # plot correlations 
```

This shows us that `hypox` and `hypot` would not be useful auxiliary variables for imputing `ct`. Depending on the minimum required correlation, `tsah` could be useful, while `edh` has the strongest correlation with `ct` out of all the variables in the data and should definitely be included in the imputation model. For the imputation of `pupil`, none of the potential auxiliary variables has a very strong relation, but `hypots` could be used. We conclude that we can exclude `hypox` from the data, since this is neither an analysis model variable nor an auxiliary variable for imputation:
```{r}
dat <- select(dat, !hypox)  # remove variable
dat <- mutate(dat, motor_score = as.factor(motor_score))
```

<!-- We'll highlight one other variable to illustrate, but ideally one would inspect all relations. The questions we'll ask are: 'Does the missing data of pupil popularity (`popular`) depend on observed teacher popularity (`texp`)?'. This can be evaluated statistically, but visual inspection usually suffices. We'll make a histogram of `texp` separately for the pupils with known popularity and missing popularity. -->

## Complete case analysis 

As previously stated, complete case analysis lowers statistical power and may bias results. The complete case analysis estimates are:
```{r cca}
fit <- glmer(mod, family = "binomial", data = na.omit(dat)) # fit the model
tidy(fit, conf.int = TRUE, exponentiate = TRUE)             # print estimates
```

As we can see, a higher `ct` (Marshall Computerized Tomography classification) is associated with a lower odds of 6-month mortality, given by the odds ratio exp(0.42), CI ... to ..., when controlling for...

<!-- TODO: fill in results above -->

<!-- If we run the multilevel model on the incomplete date, we get a warning about unindentifyability. This means that with CCA as missing data method, we cannot trust the estimates. To still obtain some estimates to compare later, we fit a simplified analysis model: using the cluster variable as indicator in the model. -->

## Imputation model

<!-- Table 7.1: Questions to gauge the complexity of a multilevel imputation task. -->
<!-- 1.	Will the complete-data model include random slopes? -->
<!-- 2.	Will the data contain systematically missing values? -->
<!-- 3.	Will the distribution of the residuals be non-normal? -->
<!-- 4.	Will the error variance differ over clusters? -->
<!-- 5.	Will there be small clusters? -->
<!-- 6.	Will there be a small number of clusters? -->
<!-- 7.	Will the complete-data model have cross-level interactions? -->
<!-- 8.	Will the dataset be very large? (fimd, section 7.3)-->

<!-- The first imputation model that we'll use is likely to be invalid. We do *not* use the cluster identifier `name` as imputation model predictor. With this model, we ignore the multilevel structure of the data, despite the high ICC. This assumes exchangeability between units. We include it purely to illustrate the effects of ignoring the clustering in our imputation effort. We'll use the default imputation methods in `mice()` (predictive mean matching to impute the continuous variables and logistic regression to impute binary variables).  -->

<!-- --- -->

<!-- \begin{center} -->

<!-- Updated until here! -->

<!-- \end{center} -->

<!-- --- -->
Mutate data to get the right data types for imputation (e.g. integer for clustering variable).

```{r}
dat <- dat %>% mutate(across(everything(), as.integer))
```


Create a methods vector and predictor matrix, and make sure `name` is not included as predictor, but as clustering variable:
```{r impact}
meth <- make.method(dat) # methods vector
pred <- quickpred(dat)   # predictor matrix
plot_pred(pred, rotate = TRUE)

pred[pred == 1] <- 2
pred["mort", ] <- 2
pred[, "mort"] <- 2
pred[c("name", "type", "age", "motor_score", "mort"), ] <- 0
pred[, "name"] <- -2
diag(pred) <- 0
plot_pred(pred, rotate = TRUE)

meth <- make.method(dat)
meth
```

Impute the incomplete data
```{r imp_impact}
imp <- mice(dat, method = meth, predictorMatrix = pred, printFlag = FALSE)
```

Evaluate the convergence of the algorithm:
```{r impact-conv, echo=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
plot_trace(imp)
```

Analyze the imputed data:
```{r impact-fit}
fit <- imp %>% 
  with(glmer(
    mort ~ type + age + as.factor(motor_score) + pupil + ct + (1 | name),
    family = "binomial"
    )) 
# tidy(pool(fit))
# as.mitml.result(fit)
# testEstimates(as.mitml.result(fit))
```
The estimated effects after imputation are presented in Table XYZ.


# Case study III: obesity data

In this example, we demonstrate a multilevel imputation of random intercept and random slope model with a continuous response. We utilize the obesity dataset included in the `micemd`\@ package, a simulated dataset that emulates an electronic survey in which individuals are asked to provide information about their weight and consumption habits in different countries. We simulate data for 5 clusters so that the true values are known. We use the following variables from the dataset:

-   **Cluster:** Region of the patients' healthcare provider (Cluster variable),
-   **Gender:** Subjects' Gender (0=male, 1=female),
-   **Age:** Subjects' age,
-   **Height:** Subjects' height in metres,
-   **Weight:** Subjects' weight in kilograms,
-   **BMI:** Subjects' body mass index,
-   **FamOb:** Family obesity history (yes or no),
-   **Time:** Response time in minutes (exclusion-restriction variable).

In this dataset, Age and FamOb are MAR, while the weight variable is affected by selection bias, attributed to an indirect MNAR mechanism. This MNAR mechanism typically arises when an unobserved or omitted variable influences both the value of the incomplete variable (in this case, Weight) and its likelihood of being missing (denoted as R).

In the primary analysis model, BMI serves as the dependent variable, with Age, Gender, and FamOb as predictors. Because of the clustered nature of the data, which is quantified with the Intraclass Correlation Coefficient (ICC) below, we include random intercepts, as well as a random slope for the Age variable.
The model is represented as:
\begin{equation}
\label{eqn:main}
BMI_{ij}= (\beta_{o}+ b_{oj} ) + (\beta_{1}+ b_{oj})* Age_{ij} + \beta_{2}*FamOb_{ij}+ \beta_{3}Gender_{ij} + \epsilon_{ij}
\end{equation}


```{r, include=FALSE, eval=TRUE}
library(here)
library(micemd)
source(here("heckman","ipdfunction.R"),local =knitr::knit_global())
```

We start by loading the data:

```{r}
data("Obesity", package = "micemd")

```

Now, let's begin by examining the missing patterns in the data by cluster:

```{r, dpi = 300,out.width="70%", fig.cap="Missing pattern", eval=TRUE}

#ggmice::plot_pattern(data=Obesity,cluster="Cluster") does not work!! bug
#ggmice::plot_pattern(data=Obesity,cluster=Obesity$Cluster) does not work!! bug
library(ggpubr)
myplots <- lapply(1:5, function(i) {
  ggmice::plot_pattern(setDT(Obesity)[Cluster==i])+
  ggplot2::ggtitle(paste0("Cluster", i))
 })
ggarrange(myplots[[1]], myplots[[3]], nrow=1,common.legend = TRUE, legend="bottom")
```

We observe that the missing pattern is non-monotonic and quite similar across the clusters. However, regarding the weight variable, we notice that is systematically missing in cluster 3. In order to evaluate
if we require a imputation method that accounts for clustering  we assess the Intraclass Correlation

```{r obesity-icc, eval=TRUE}
Nulmodel <- lme4::lmer(BMI ~ 1 + (1|Cluster), data = Obesity)
performance::icc(Nulmodel)
```

Since the ICC is above 0.1 and as the main analysis will be use a mixed model, we decide
to use two-level (2l) imputation methods. In this imputation process, we include all predictor
variables from equation \ref{eqn:main} in the main model. However, since BMI is a composite of
weight and height, we use deterministic imputation for these, which is described below.

We use the **find.defaultMethodfunction** provided in the **micemd** package, which suggests an appropriate method for MAR variables based on the type of variable, number of observations in the cluster, and number of clusters.

It suggests using '2l.2stage.bin' for the FAV variable and '2l.2stage.norm' for the age variable. However, after inspecting the age density plot, we consider modifying its method to
'2l.2stage.pmm'. For the BMI variable, we employ deterministic imputation.

```{r obesitymeth, eval=TRUE}
library(micemd)
meth_mar <- micemd::find.defaultMethod(Obesity, ind.clust=1, I.small = 7,
                               ni.small = 100, prop.small = 0.4)
meth_mar["BMI"]<- "~ I(Weight / (Height)^2)"
meth_mar["Age"]<-"2l.2stage.pmm" 
```

For these imputation models, it is necessary to specify the prediction matrix, with the cluster
variable labelled as -2 and the predictor variable measured within clusters labelled as 2, encompassing all variables. We need to suprime the variable Time as this variable is not specified in the main model. We also  modify the relationship between BMI, weight and height in the prediction matrix to avoid circular predictions. Then we proceed to run the imputation model.
```{r obesity-predmar,out.width="70%", eval=TRUE}
pred_mar <- mice(Obesity, maxit = 0)$pred 
pred_mar[,"Cluster"] <- -2 # clustering variable
pred_mar[,"Time"] <- 0
pred_mar[pred_mar==1] <- 2
pred_mar[c("Height", "Weight"), "BMI"] <- 0
ggmice::plot_pred(pred_mar)
imp_mar <- mice::mice(data = Obesity, meth = meth_mar, pred = pred_mar,
                m=10, seed = 123, printFlag = FALSE)
```

```{r obesity-predmar1,eval=TRUE}
summary(complete(imp_mar,"long")$Weight)
```
We are also contemplating the utilisation of the predictive mean matching (pmm) option, as the values imputed using a fully parametric method may be implausibly low for some patients.

```{r obesity-predmar_pmm, eval=TRUE}
meth_mar["Weight"]<-"2l.2stage.pmm" 
imp_mar_pmm <- mice(data = Obesity, meth = meth_mar, pred = pred_mar,
                    m=10, seed = 123, printFlag = FALSE)
```

```{r obesity-predmar_pmm1,eval=TRUE}
summary(complete(imp_mar_pmm,"long")$Weight)
ggmice::plot_trace(imp_mar_pmm, "Weight")
```

After confirming convergence, we proceed to save the results for future use.  We consider the possibility that patients may not have been selected randomly, which would then have led to a distribution for weight that does not reflect the weight in the population. It???s likely that an omitted variable, like self-esteem, could influence this selection. For instance, individuals with lower self-esteem might have higher weight values, impacting their willingness to provide honest information due to embarrassment.

To address this situation, two approaches have been proposed for dealing with Missing Not at Random (MNAR) data: pattern-mixed models and selection models. Within pattern-mixed models, methods like the delta method and more advanced ones like NARFS have been suggested. The selection model approach includes methods such as the Heckman model, which can be particularly useful in this case. Several methods, including those by \cite{galimard2018}, and the recently a Heckman method designed for two-level data, allow for variations in intercepts and exposure effects (random intercept and slope) \cite{munoz2023}.

To apply the **2l.2stage.heckman** method, the weight variable should be specified as '2l.2stage.heckman' found in the micemd package. Additionally, the prediction matrix needs modification because this method involves specifying two equations: one for the outcome, describing the incomplete variable in terms of partially observed predictors (in this case, all variables from the main model), and the other for the selection model, explaining the probability of being observed based (R) on certain variables.
For the outcome equation we consider the same imputation model that we used for
the MAR case (main model).
 $$Weight_{ij}= \beta^O_{o} + \beta^O_{1}Age_{ij} + \beta^O_{2}FamOb_{ij}+ \beta^O_{3}Gender_{ij} + \epsilon^O_{ij}$$ 
Regarding the selection equation, we include the same predictors as those in the main model, as well as a time variable. Here the time variable serves as a restriction exclusion variable specifically explaining the probability of being observed but not affecting the incomplete value (Weight). In this context, we assume that the time a user spends completing the survey serves as a proxy for the barriers they may encounter in survey completion, such as familiarity with the survey content or internet speed. These factors may lead the user to skip specific questions or even the entire survey. Also, we assume the time does not have any influence on the subject???s weight.
 $$R_{ij}= \beta^S_{o} + \beta^S_{1}Age_{ij} + \beta^S_{2}FamOb_{ij}+ \beta^S_{3}Gender_{ij} +\beta^S_{4}Time_{ij}+ \epsilon^S_{ij}$$ 

These two equations are jointly estimated under the assumption that the error terms are interconnected with a bivariate normal distribution. For a more comprehensive understanding of the model and the exclusion restriction, see \cite{munoz2023a}.

To use information from both equations, we must adjust the prediction matrix. The cluster variable remains specified as before (-2). In this imputation method, all the variables present in both the selection and outcome equations are included with a random effect.

However, it is essential to distinguish which of these variables appear in each equation. In this framework, when a variable is shared between both equations, it is denoted as (2). Predictors exclusive to the outcome equation are indicated as (-4), while those exclusive to the selection equation are labelled as (-3). Consequently, the only alteration needed in the predictor matrix pertains to the variable 'Time'.

```{r obesity-predmnar, eval=TRUE}
pred_mnar <- pred_mar
pred_mnar["Weight","Time"]<- -3
ggmice::plot_pred(pred_mnar)
```

We also need to modify the method of the weight variable.

```{r obesity-predmnar1, eval=TRUE}
meth_mnar <- meth_mar
meth_mnar["Weight"]<- "2l.2stage.heckman"
```
Then we proceed to run the imputation model as before, after executing these imputation procedures, it is essential to assess convergence and the coherence of the imputed values.
```{r obesity-predmnar2,eval=TRUE}
imp_mnar<- mice(data = Obesity, meth = meth_mnar, pred = pred_mnar,
                m=10, seed = 123, printFlag = FALSE)
summary(complete(imp_mnar,"long")$Weight)
```

Upon examining the weight variable, we noticed that the imputed range falls outside the realm of plausible values (as weight should be positive).

```{r obesity-predmnar3,eval=TRUE}
summary(complete(imp_mnar,"long")$Weight)
```
Consequently, as before we  use the 'pmm', option but this time for the Heckman imputation, this approach ensures that the imputed values remain within the range of observable values. We then run the imputation model but this time using the option of pmm, to assure that weight values are in the range of the observable data, this can be implemented by setting the pmm parameter to true.

```{r obesity-predmnarp, eval=TRUE}
imp_mnar_pmm <- mice(data = Obesity, meth = meth_mnar, pred = pred_mnar,
                     m=10, seed = 123, pmm = T,  printFlag = FALSE)
```

We check the convergency of the results
```{r obesity-predmnarp1, eval=TRUE}
summary(complete(imp_mnar_pmm,"long")$Weight)
ggmice::plot_trace(imp_mar_pmm, "Weight")
```

After this modification we proceed to compare the effects on the model. We
run the analysis model on each of the completed datasets as well asthe dataset where the
incomplete values are removed (Complete Case analysis, CC).

```{r models, eval=TRUE}
library(ggplot2)
cc_rs<- with(setDT(Obesity)[complete.cases(Obesity),],
             lme( BMI ~ Age + FamOb + Gender, random=~1+Age|Cluster))
mar_rs <- with(imp_mar,lme( BMI ~ Age + FamOb + Gender,random=~1+Age|Cluster))
mar_pmm_rs <- with(imp_mar_pmm,lme( BMI ~ Age + FamOb + Gender,random=~1+Age|Cluster))
mnar_rs<- with(imp_mnar,lme(BMI ~ Age + FamOb + Gender,random=~1+Age|Cluster))
mnar_pmm_rs<- with(imp_mnar_pmm, lme(BMI ~ Age + FamOb + Gender,random=~1+Age|Cluster))
list_models<-list(cc_rs,mar_rs,mar_pmm_rs,mnar_rs,mnar_pmm_rs)
plot_models(list_models,
            mod_name = c("Complete case", "MAR","MAR_pmm", "MNAR", "MNAR_pmm"))
```

We note that there is minimal disparity in the age effect, FamObs, or  Gender across the various imputation models under consideration. An analysis of the intercept reveals that, under the MNAR assumption, a higher average BMI is anticipated compared to the MAR assumption.
Nonetheless, with respect to precision of estimates, we notice that in general MNAR imputation leads to wider confidence intervals, in this case it does not have any influence on the final result but there could be cases where variation in the assumed missing mechanism could lead also to differences on significant test and therefore lead to contradictory conclusions.   



# Conclusion
This paper is dedicated to exploring the imputation process for incomplete datasets, with a primary focus on utilizing a hierarchical model for analysis. Initially, users are encouraged to consider the main analysis within the context of the incomplete dataset, along with insights provided by domain experts, to gain a better understanding of variable relationships. Employing clear data visualization is instrumental in comprehending the missing data patterns, establishing a missing mechanism, and aiding in the selection of suitable imputation methods.

The "Mice" and "mice"-based R packages offer a range of imputation methods tailored for hierarchical data, easily adaptable to the dataset's structure. Before proceeding with the analysis of the imputed dataset, it is essential to assess the convergence of the imputation method. This evaluation can reveal issues such as circular problems, the need for additional iterations, or challenges associated with the chosen imputation method.


# Funding

This project has received funding from the European Union's Horizon 2020 research and innovation programme under ReCoDID grant agreement No 825746.

The views expressed in this paper are the personal views of the authors and may not be understood or quoted as being made on behalf of or reflecting the position of the regulatory agency/agencies or organizations with which the authors are employed/affiliated.

# References

